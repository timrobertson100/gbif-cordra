# GBIF Cordra

This contains _experimental_ code of using the [Cordra](https://cordra.org/) object store to front the GBIF.org infrastructure.

A demonstration installation serving 200M Specimen records can be found on http://cordra.gbif.org.  


Currently, this includes:

1. [hbase-storage](./hbase-storage) allowing HBase to be used for the object store (uses modulus based salting on row keys)
2. [gbif-cordra-utils](./gbif-cordra-utils) with samples for interacting with Cordra through the API
3. [gbif-backfill](./gbif-backfill) using Spark to efficiently bulk load HBase and ElasticSearch stores

The initial focus is on efficient loading of Cordra, and future work on the actual data model is necessary.

To build this project use `mvn package`

## Setup Cordra

_This is currently a manual process while ideas are being explored_

To set up, or recreate HBase use the following, adapted as necessary:
```
disable 'cordra'
drop 'cordra'
create 'cordra', 
  {NAME => 'm', VERSIONS => 1, COMPRESSION => 'SNAPPY', DATA_BLOCK_ENCODING => 'FAST_DIFF', BLOOMFILTER => 'NONE'},  
  {SPLITS => [
    '01','02','03'
  ]}
```

Cordra setup notes:

1. Use Tomcat
2. Copy in the [Cordra war](https://repo1.maven.org/maven2/net/cnri/cordra/cordra/2.3.2/cordra-2.3.2.war) and let it expand
3. Remove `protobuf-java-3.12.2.jar` from lib as it classes with the one in HBase
4. Copy the built `cordra-hbase-storage-1.0-SNAPSHOT-shaded.jar` to the `../webapps/cordra-2.3.2/WEB-INF/lib/` 
5. Create a Cordra data directory (e.g. `/data/cordra`)
6. Copy the `setenv.sh` to `tomcat/bin` changing data directory to that created above
7. Copy the repoinit.json and config.json to the Cordra data dir
8. Start tomcat

On startup Cordra will create the Elasticsearch index, and create the original startup objects in HBase. 

The ES index can be deleted using - e.g.
 
```
curl -X DELETE "http://uc6n10.gbif.org:9200/cordra?pretty"
```

## Bulk load objects

First we create a Cordra data type for the object. Logging in as an admin use the menu `/admin/types` and 
create new one named Occurrence with the following schema:

```
{
  "type": "object",
  "required": [
    "scientificName"
  ],
  "properties": {
    "id": {
      "type": "string",
      "cordra": {
        "type": {
          "autoGeneratedField": "handle"
        }
      }
    },
    "scientificName": {
      "type": "string",
      "maxLength": 512,
      "title": "Scientific name",
      "cordra": {
        "preview": {
          "showInPreview": true,
          "isPrimary": true
        }
      }
    }
  }
}
```

Bulk loading requires 3 steps.

### 1: Generate objects

A spark script connects to the GBIF Hive metastore, and using Cordra libraries creates the JSON for the 
object to store in HBase, and the ES JSON doc. These are stored in an Avro file and contain fake transactionIDs 
for Cordra. 

Example: 
``` 
hdfs dfs -rm -r /tmp/cordra
spark2-submit --class org.gbif.cordra.backfill.GenerateObjects  \ 
  --conf spark.dynamicAllocation.enabled=false  \
  --master yarn \
  --executor-memory 16G \
  --executor-cores 5 \
  --num-executors 15 \
  gbif-backfill-1.0-SNAPSHOT-shaded.jar \
  --hive-db uat \
  --target-dir hdfs:///tmp/cordra
```

### 2: Index objects in Elasticsearch

A spark script loads the JSON docs and indexes them directly in ES.

Example:  
```
spark2-submit --class org.gbif.cordra.backfill.IndexToES  \
  --conf spark.dynamicAllocation.enabled=false  \
  --master yarn \
  --executor-memory 8G \
  --executor-cores 2 \
  --num-executors 10 \
  gbif-backfill-1.0-SNAPSHOT-shaded.jar \
  --source-dir hdfs:///tmp/cordra \
  --es-nodes uc6n10.gbif.org,uc6n11.gbif.org,uc6n12.gbif.org \
   --es-port 9200 \
  --es-index cordra
```

### 3: Bulk load objects to HBase

A spark script generates HFiles from the JSON to bulk load to HBase.

Example (take care to match region count to the same configuration as cordra):
```
spark2-submit --class org.gbif.cordra.backfill.GenerateHFiles \
  --conf spark.dynamicAllocation.enabled=false \
  --master yarn \
  --executor-memory 8G \
  --executor-cores 2 \
  --num-executors 10 \
  gbif-backfill-1.0-SNAPSHOT-shaded.jar \
  --source-dir hdfs:///tmp/cordra \
  --hbase-table cordra \
  --hbase-regions 3 \
  --hbase-zk c4zk1.gbif-uat.org,c4zk2.gbif-uat.org,c4zk3.gbif-uat.org \
  --hfile-dir /tmp/cordra-hfiles
```   

These can then be bulk loaded into HBase:

```
sudo -u hdfs hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles \
  -Dcreate.table=no /tmp/cordra-hfiles cordra
```

The objects will only be available to authenticate users in Cordra. To make them available to all, use the
admin/authorization menu, and add the following to the `schemaAcls`:

```
    "Occurrence": {
      "defaultAclRead": [
        "public"
      ],
      "defaultAclWrite": [],
      "aclCreate": []
    }
```

     

